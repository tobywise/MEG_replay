{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay in Aversive Environments - MEG Preprocessing\n",
    "\n",
    "#### _This is a template that will be parameterised and run via [Papermill](http://papermill.readthedocs.io/) for each subject_\n",
    "\n",
    "This notebook performs preprocessing of localiser and task data. \n",
    "\n",
    "Preprocessing steps:\n",
    "\n",
    "1. Identification and loading of raw data\n",
    "2. Maxwell filtering\n",
    "3. Filtering\n",
    "4. Epoching\n",
    "5. Downsampling\n",
    "6. ICA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.io import read_raw_ctf\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.preprocessing import ICA, create_eog_epochs, create_ecg_epochs\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import yaml\n",
    "import papermill as pm\n",
    "import pandas as pd\n",
    "np.random.seed(100)\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/'  # Directory containing data\n",
    "session_id = 'MG05513'  # ID of the scanning session\n",
    "n_runs = 9  # Number of runs\n",
    "output_dir = 'data/derivatives'  # Where the output data should go\n",
    "eye_tracking = True  # If True, eye-tracking measures will be used for exclusion of blink-related ICA components\n",
    "n_stim = 8  # Number of stimuli\n",
    "cores = 1  # Number of cores to use for parallel processing\n",
    "blink_components = None\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = str(cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n",
    "\n",
    "Data is stored in [BIDS format](https://www.nature.com/articles/sdata2018110) - currently MNE doesn't directly read from BIDS however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directory for this subject\n",
    "data_dir = os.path.join(data_dir, 'sub-{0}'.format(session_id), 'ses-01', 'meg')\n",
    "\n",
    "# Find all files in the directory and make sure they're in the right order (i.e. ascending)\n",
    "data = os.listdir(data_dir)\n",
    "data = sorted([i for i in data if '.ds' in i and str(session_id) in i])\n",
    "\n",
    "# Check we have the right number of runs\n",
    "assert len(data) == n_runs, \"Wrong number of data files, found {0}\".format(len(data))\n",
    "\n",
    "# See what has been found\n",
    "print(data)\n",
    "\n",
    "# Get all the data and read it in\n",
    "raws = []\n",
    "run_idx = range(0, n_runs)\n",
    "\n",
    "# Read in each data set\n",
    "for i in run_idx:\n",
    "    start_time = time.time()\n",
    "    raws.append(read_raw_ctf(os.path.join(data_dir, data[i]), preload=True))\n",
    "    time_taken = time.time() - start_time\n",
    "    print(\"Time taken = {0}\".format(str(datetime.timedelta(seconds=time_taken))))\n",
    "\n",
    "# Concatenate the runs\n",
    "raw = mne.concatenate_raws(raws)\n",
    "del raws  # delete the list of raw data to conserve memory\n",
    "\n",
    "# Label eye-tracking channels as EOG\n",
    "raw.set_channel_types({'UADC001-2910': 'eog', 'UADC002-2901': 'eog', 'UADC003-2901': 'eog'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Maxwell filter\n",
    "This is partly because some MNE functions don't seem to work properly if CTF compensation is turned on, however Maxwell filtering also appears to do a better job than CTF compensation at removing noise from movement etc (https://martinos.org/mne/stable/auto_tutorials/plot_brainstorm_phantom_ctf.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.apply_gradient_compensation(0)  # Remove CTF compensation\n",
    "mf_kwargs = dict(origin=(0., 0., 0.), st_duration=10.)\n",
    "raw = mne.preprocessing.maxwell_filter(raw, **mf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "\n",
    "Bandpass filter between 0.5hz and 45hz, using windowed FIR filter with MNE default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FILTERING\")\n",
    "raw.filter(0.5, 50, method='fir', fir_design='firwin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA\n",
    "\n",
    "ICA is performed on the raw data and is set to find the number of components that explains 95% of the variance. \n",
    "\n",
    "We don't do much in terms of selecting noise-related components here - the data is generally pretty clean and doesn't seem to benefit much from extra denoising, so we simply automatically detect blink-related components based on eye-tracking channels.\n",
    "\n",
    "For some subjects eye tracking was poor due to equipment problems. If we're not able to identify a blink-related component automatically that may be due to an absent eye tracking channel, or just due to noisy eye tracking data - in this case the notebook raises an error and we can rerun with manually selected components to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ICA\n",
    "picks_meg = mne.pick_types(raw.info, meg=True, ref_meg=False)\n",
    "reject = dict(mag=5e-12, grad=4000e-13)\n",
    "ica = ICA(n_components=0.95, method='fastica',\n",
    "          random_state=100, max_iter=100).fit(raw, decim=10, picks=picks_meg, reject=reject)\n",
    "\n",
    "# Plot components\n",
    "ica.plot_components()\n",
    "\n",
    "# Save decomposition\n",
    "ica.save(os.path.join(output_dir, 'ICA', 'sub-{0}_ses-01_task-AversiveLearningReplay_proc-ICA.fif.gz').format(session_id))\n",
    "\n",
    "# Find blink-related components\n",
    "if blink_components is None or blink_components == 'None':\n",
    "    blink_components, scores = ica.find_bads_eog(raw, threshold=1.5)\n",
    "    ica.plot_scores(scores, exclude=blink_components, labels='blink')\n",
    "    show_picks = np.abs(scores).argsort()[::-1][:5]\n",
    "    ica.plot_components(blink_components, colorbar=True)\n",
    "    \n",
    "# Find ECG componenhts\n",
    "ecg_epochs = create_ecg_epochs(raw, tmin=-.5, tmax=.5, picks=picks_meg)\n",
    "\n",
    "ecg_components, scores = ica.find_bads_ecg(ecg_epochs, method='ctps')\n",
    "ica.plot_scores(scores, exclude=ecg_components, labels='ecg')\n",
    "ecg_components = ecg_components[:3]\n",
    "\n",
    "if isinstance(blink_components, list):\n",
    "    \n",
    "    print(\"APPLYING ICA\")\n",
    "\n",
    "    # Only select a maximum of 2 components\n",
    "    blink_components = blink_components[:2]\n",
    "\n",
    "    ica.exclude = blink_components + ecg_components\n",
    "    ica.apply(raw)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"No blink related components identified automatically - do this manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find events\n",
    "\n",
    "This identifies triggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINDING EVENTS\")\n",
    "events = mne.find_events(raw, stim_channel='UPPT001', shortest_event=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create epochs\n",
    "\n",
    "Here we split the continuous data into epochs. Stimulus triggers are from 2 to the number of stimuli * 2 with a step of 2 (this is because sending odd numbers also triggers shocks in the actual task). Code 99 is used for null trials (only used in the localiser). We don't reject any trials because we want to decide on rejections later.\n",
    "\n",
    "We're only selecting the planning and rest periods from the task here as these are the periods we'll be looking for replay during."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EPOCHING\")\n",
    "\n",
    "# Get event names\n",
    "# Event numbers for the localiser are even numbers from 2 up to 2 * the number of stimuli\n",
    "localiser_event_names = dict([('stimulus_{0}'.format(i), i) for i in list(range(2, n_stim * 2, 2))] + [('null', 99)])\n",
    "\n",
    "# We get task event numbers from the task config file\n",
    "with open('task/replay_task_settings.yaml', 'rb') as f:\n",
    "    task_config = yaml.load(f)\n",
    "task_event_names = task_config['triggers']\n",
    "task_event_names['final_state'] = 26\n",
    "\n",
    "# Sometimes we record a rest event before trials begin for some reason - if this happens we need to remove this.\n",
    "if np.where(events[:, 2] == task_event_names['rest'])[0][0] < np.where(np.isin(events[:, 2], [task_event_names['planning'], task_event_names['outcome_only_warning']]))[0][0]:\n",
    "    events = np.delete(events, np.where(events[:, 2] == task_event_names['rest'])[0][0], axis=0)\n",
    "    \n",
    "# Occasionally planning triggers get coded as 62 instead of 60, so change any 62s to 60s. This might be just in cases with the incorrect trigger timing (M200-203)\n",
    "events[:, 2][events[:, 2] == 62] = 60\n",
    "events[:, 2][events[:, 2] == 98] = 99 # 98 for null events for one subject these got recorded as 98 rather than 99 for no apparent reason\n",
    "\n",
    "# Create the epoch objects\n",
    "localiser_epochs = mne.Epochs(raw, events[np.isin(events[:, 2], list(localiser_event_names.values()))], tmin=-0.1, tmax=0.8, preload=True, event_id=localiser_event_names,\n",
    "                              reject=None)\n",
    "task_epochs = mne.Epochs(raw, events[np.isin(events[:, 2], list(task_event_names.values()))], \n",
    "                         tmin=0, tmax=np.max([task_config['MEG_durations']['start_duration'], task_config['MEG_durations']['rest_duration']]), \n",
    "                         preload=True, event_id={k: task_event_names[k] for k in ('planning', 'rest', 'outcome_only_warning')}, reject=None)\n",
    "\n",
    "# Outcome + rest epochs\n",
    "outcome_rest_epochs = mne.Epochs(raw, events[np.isin(events[:, 2], list(task_event_names.values()))], \n",
    "                         tmin=-0.1, tmax=8, \n",
    "                         preload=True, event_id={k: task_event_names[k] for k in ('shock_outcome', 'no_shock_outcome')}, reject=None)\n",
    "\n",
    "# Epochs for other task events\n",
    "final_state_epochs = mne.Epochs(raw, events[np.isin(events[:, 2], list(task_event_names.values()))], \n",
    "                         tmin=0, tmax=task_config['MEG_durations']['shock_symbol_delay'], \n",
    "                         preload=True, event_id={k: task_event_names[k] for k in ('final_state', 'outcome_only_outcome')}, reject=None)\n",
    "outcome_epochs = mne.Epochs(raw, events[np.isin(events[:, 2], list(task_event_names.values()))], \n",
    "                         tmin=-0.1, tmax=task_config['MEG_durations']['shock_delay'], \n",
    "                         preload=True, event_id={k: task_event_names[k] for k in ('shock_outcome', 'no_shock_outcome')}, reject=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LOCALISER EVENTS\")\n",
    "\n",
    "for k, v in localiser_event_names.items():\n",
    "    print(\"Number of {0} events = {1}\".format(k, np.sum(events[:, 2] == v)))\n",
    "    \n",
    "print(\"TASK EVENTS\")\n",
    "\n",
    "for k, v in task_event_names.items():\n",
    "    print(\"Number of {0} events = {1}\".format(k, np.sum(events[:, 2] == v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for n, i in enumerate(np.unique(events[:, 2])):\n",
    "    data.append(go.Scatter(x=events[events[:, 2] == i][:, 0], y=[n] * len(events[events[:, 2] == i][:, 0]), mode = 'markers'))\n",
    "    \n",
    "\n",
    "layout = dict(title='Events', showlegend=False,\n",
    "              xaxis=dict(title='Samples'), yaxis=dict(title='Event ID', tickvals=np.arange(len(np.unique(events[:, 2]))), ticktext=[str(i) for i in np.unique(events[:, 2])]), \n",
    "              width=1500, height=600)\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig, filename='events')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the events look right\n",
    "\n",
    "First, we should have 600 localiser events (or thereabouts - a couple of runs got cut short by a trial or two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(localiser_epochs) > 590, 'Unexpected number of localiser trials, found {0}, expected 600'.format(len(localiser_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load in the behavioural data to compare the number of recorded trials there to what we find in the MEG triggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids = pd.read_csv('subject_ids.csv', sep='\\t')\n",
    "behavioural_id = subject_ids.loc[subject_ids['meg'] == session_id]['behavioural'].values[0]\n",
    "behavioural_data = pd.read_csv(os.path.join('task/Data/behavioural', [i for i in os.listdir('task/Data/behavioural') if behavioural_id in i][0]))\n",
    "behavioural_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we iterate over trials in the behavioural data and check whether the trial number is `NaN`. If the experiment is paused for any reason (e.g. electrodes coming loose), we record a `NaN` trial and the experiment subsequently restarts from this trial when unpaused - this allows us to remove any triggers from a trial that was restarted by identifying these trials in the behavioural data and removing associated planning/rest epochs from the MEG data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial in range(len(behavioural_data)):\n",
    "    if np.isnan(behavioural_data['trial_number'][trial]):  # check whether this is nan (trial where the task was paused)\n",
    "        print(\"Dropping trial {0}\".format(trial))\n",
    "        if task_epochs[trial * 2 + 1]._name == 'rest':\n",
    "            task_epochs = task_epochs.drop(trial * 2 + 1)\n",
    "        task_epochs = task_epochs.drop(trial * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of planning and outcome only trials should now add up to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_planning_oo_trials = len(task_epochs['planning']) + len(task_epochs['outcome_only_warning'])\n",
    "assert n_planning_oo_trials == 100, 'Number of planning + outcome only trials is {0}, expected 100'.format(n_planning_oo_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample\n",
    "\n",
    "Resampling to 100hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del raw  # Delete the raw data variable to save memory\n",
    "\n",
    "print(\"DOWNSAMPLING\")\n",
    "print('Original sampling rate: {0} Hz'.format(localiser_epochs.info['sfreq']))\n",
    "localiser_epochs = localiser_epochs.copy().resample(100, npad='auto')\n",
    "task_epochs = task_epochs.copy().resample(100, npad='auto')\n",
    "final_state_epochs = final_state_epochs.copy().resample(100, npad='auto')\n",
    "outcome_epochs = outcome_epochs.copy().resample(100, npad='auto')\n",
    "outcome_rest_epochs = outcome_rest_epochs.copy().resample(100, npad='auto')\n",
    "print('Downsampled sampling rate: {0} Hz'.format(localiser_epochs.info['sfreq']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the epoched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localiser_epochs.save(os.path.join(output_dir, 'preprocessing/localiser', 'sub-{0}_ses-01_task-AversiveLearningReplay_run-localiser_proc_ICA-epo.fif.gz').format(session_id))\n",
    "task_epochs.save(os.path.join(output_dir, 'preprocessing/task', 'sub-{0}_ses-01_task-AversiveLearningReplay_run-task_proc_ICA-epo.fif.gz').format(session_id))\n",
    "final_state_epochs.save(os.path.join(output_dir, 'preprocessing/task', 'sub-{0}_ses-01_task-AversiveLearningReplay_run-task_final_state_proc_ICA-epo.fif.gz').format(session_id))\n",
    "outcome_epochs.save(os.path.join(output_dir, 'preprocessing/task', 'sub-{0}_ses-01_task-AversiveLearningReplay_run-task_outcome_proc_ICA-epo.fif.gz').format(session_id))\n",
    "outcome_rest_epochs.save(os.path.join(output_dir, 'preprocessing/task', 'sub-{0}_ses-01_task-AversiveLearningReplay_run-task_outcome_rest_proc_ICA-epo.fif.gz').format(session_id))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "psychopy3"
  },
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "mne"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nteract": {
   "version": "0.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
