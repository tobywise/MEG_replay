{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay in Aversive Environments - Localiser decoding\n",
    "\n",
    "#### _This is a template that will be parameterised and run via [Papermill](http://papermill.readthedocs.io/) for each subject_\n",
    "\n",
    "This notebook trains a classifier on the localiser data to identify the neural signature associated with each image in the task.\n",
    "\n",
    "Classification steps:\n",
    "\n",
    "1. Loading preprocessed data\n",
    "2. Running classification over multiple trial timepoints to generate a decoding timecourse\n",
    "3. Hyperparameter estimation\n",
    "4. Producing a confusing matrix to assess classifier performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('..')\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'code')\n",
    "\n",
    "from mne.io import read_raw_ctf\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.preprocessing import ICA, create_eog_epochs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import FunctionTransformer, scale\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.externals import joblib\n",
    "from scipy.stats import halfcauchy\n",
    "from mne.decoding import (SlidingEstimator, cross_val_multiscore)\n",
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import os\n",
    "import papermill as pm\n",
    "from utils import add_features, select_timepoints\n",
    "from plotting import plot_confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "session_id = 'MG05572'  # ID of the scanning session\n",
    "output_dir = 'data/derivatives'  # Where the output data should go\n",
    "n_stim = 8  # Number of stimuli, including null\n",
    "shifts = [-5, 6]  # Additional timepoints to use as features\n",
    "n_iter_search = 100  # Number of iterations of the random search parameter optimisation procedure\n",
    "cores = 1  # Number of cores to use for parallel processing\n",
    "os.environ['OMP_NUM_THREADS'] = str(cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localiser_epochs = mne.read_epochs(os.path.join(output_dir, 'preprocessing/localiser', 'sub-{0}_ses-01_task-AversiveLearningReplay_run-localiser_proc_ICA-epo.fif.gz').format(session_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the responses to image stimuli in sensor space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.arange(0.06, 0.3, 0.02)\n",
    "evoked = localiser_epochs.average()\n",
    "evoked.plot_topomap(times, ch_type='mag')\n",
    "evoked.plot_topomap(0.2, ch_type='mag', show_names=True, colorbar=False, size=3, res=128);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timecourse of decoding accuracy\n",
    "\n",
    "This gives us an idea of where in time we're able to decode stimulus identity - this should be around chance at 0s before rising a little before 200ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get epoch data\n",
    "X_raw = localiser_epochs.get_data()  # MEG signals: n_epochs, n_channels, n_times (exclude non MEG channels)\n",
    "y_raw = localiser_epochs.events[:600, 2]  # Get event types\n",
    "\n",
    "# select events and time period of interest\n",
    "picks_meg = mne.pick_types(localiser_epochs.info, meg=True, ref_meg=False)\n",
    "event_selector = (y_raw < 23) | (y_raw == 99)\n",
    "X_raw = X_raw[event_selector, ...]\n",
    "y_raw = y_raw[event_selector]\n",
    "X_raw = X_raw[:, picks_meg, :]\n",
    "\n",
    "print(\"Number of unique events = {0}\\n\\nEvent types = {1}\".format(len(np.unique(y_raw)),\n",
    "                                                                  np.unique(y_raw)))\n",
    "\n",
    "# Do PCA with 50 components\n",
    "pca = UnsupervisedSpatialFilter(PCA(50), average=False)\n",
    "pca_data = pca.fit_transform(X_raw)\n",
    "\n",
    "# CLASSIFIER\n",
    "# Logistic regression with L2 penalty, multi-class classification performed as one-vs-rest\n",
    "# Data is transformed to have zero mean and unit variance before being passed to the classifier\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression(multi_class='multinomial', C=0.1, penalty='l2', class_weight=\"balanced\",\n",
    "                                                         solver='saga', max_iter=100000, tol=0.2))\n",
    "\n",
    "# Try classifying at all time points with 5 fold CV\n",
    "time_decod = SlidingEstimator(clf, n_jobs=1, scoring='accuracy')\n",
    "scores = cross_val_multiscore(time_decod, pca_data, y_raw,\n",
    "                              cv=5, n_jobs=1)\n",
    "\n",
    "# Mean scores across cross-validation splits\n",
    "mean_scores = np.mean(scores, axis=0)\n",
    "best_idx = np.where(mean_scores == mean_scores.max())[0][0]\n",
    "\n",
    "print(\"Best classification at index {0}, {1}ms\".format(best_idx, (best_idx * 10) - 133))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "# ax.plot(range(10), mean_scores, label='Score')\n",
    "ax.axhline(1. / n_stim, color='#a8a8a8', linestyle='--', label='Chance')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Subset accuracy')\n",
    "ax.axvline(.0, color='#515151', linestyle='-')\n",
    "ax.set_title('Decoding accuracy')\n",
    "ax.plot(localiser_epochs.times[:len(mean_scores)], mean_scores, label='Score')\n",
    "ax.axvline(localiser_epochs.times[best_idx], color='#76b9e8', linestyle='--')\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the response timecourse across trials\n",
    "\n",
    "This just shows the responses across our principal components across all trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = mne.EvokedArray(np.mean(pca_data, axis=0),\n",
    "                     mne.create_info(50, localiser_epochs.info['sfreq']), tmin=-0.1)\n",
    "ev.plot(show=False, window_title=\"PCA\", time_unit='s');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise hyperparameters using randomised search\n",
    "\n",
    "Optimising regularisation parameter (C) and number of PCA components. Randomised search works like grid search but rather than exhaustively searching a grid of predefined parameter values, it samples from specified parameter distributions. This is useful here because C values closer to 0 tend to be better, but this is not always the case - here we sample C values from a half-Cauchy distribution so that low values are tested more frequently, without us having to manually specify a grid that conforms to this criterion.\n",
    "\n",
    "To make the process more streamlined, we create a classifier pipeline containing the following steps:\n",
    "1. Temporal PCA (reducing dimensionality in the channel dimension)\n",
    "2. Adding features from adjacent timepoints - although we're focusing on the 200ms mark, we add timepoints from before and after this point as additional features. This tends to boost decoding accuracy by ~10%.\n",
    "3. Scaling the data to be in a standard range.\n",
    "4. Logistic regression with lasso (L2) regularisation and multinomial multi-class classification.\n",
    "\n",
    "This is the iteratively run and evaluated with 3-fold cross validation across different hyperparameter settings.\n",
    "\n",
    "All of this is performed on data from the 200ms point as this has been used successfully in previous studies - interestingly decoding from other timepoints doesn't seem to produce the kind of sequential replay we get when decoding from 200ms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Toby\\Anaconda2\\envs\\mne\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning:\n",
      "\n",
      "The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "\n",
      "C:\\Users\\Toby\\Anaconda2\\envs\\mne\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning:\n",
      "\n",
      "You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "\n",
      "C:\\Users\\Toby\\Anaconda2\\envs\\mne\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning:\n",
      "\n",
      "You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "\n",
      "C:\\Users\\Toby\\Anaconda2\\envs\\mne\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning:\n",
      "\n",
      "You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "\n",
      "C:\\Users\\Toby\\Anaconda2\\envs\\mne\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning:\n",
      "\n",
      "You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "\n",
      "C:\\Users\\Toby\\Anaconda2\\envs\\mne\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning:\n",
      "\n",
      "You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter optimisation done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set idx to 33 for 200ms\n",
    "best_idx = 33\n",
    "\n",
    "# Select data from timepoint of interest\n",
    "X, y = (X_raw.copy(), y_raw.copy())\n",
    "# X_null = X[..., :11]\n",
    "X = X[..., best_idx + shifts[0]:best_idx + shifts[1]] \n",
    "# X = X[y!=99]\n",
    "# y = y[y!=99]\n",
    "\n",
    "\n",
    "# Create a pipiline that combines PCA, feature augmentation, scaling, and the logistic regression classifier\n",
    "clf = make_pipeline(UnsupervisedSpatialFilter(PCA(50), average=False), \n",
    "                    FunctionTransformer(add_features, validate=False), StandardScaler(), \n",
    "                    LogisticRegression(multi_class='ovr', C=0.1, penalty='l1', class_weight=\"balanced\",\n",
    "                                                         solver='saga', max_iter=100000, tol=0.2))\n",
    "\n",
    "# Parameter distributions passed to the random search procedure\n",
    "param_dist = {\"unsupervisedspatialfilter__estimator__n_components\": range(30, 60),\n",
    "              \"logisticregression__C\": halfcauchy(scale=5)}\n",
    "\n",
    "# run randomized search\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=3, n_jobs=1, scoring='accuracy')\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Produce a dataframe of the search results\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "print(\"Parameter optimisation done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_epochs = mne.read_epochs(os.path.join(output_dir, 'preprocessing/task', 'sub-{0}_ses-01_task-AversiveLearningReplay_run-task_proc_ICA-epo.fif.gz').format(session_id))\n",
    "# planning_epochs = task_epochs['planning']\n",
    "# rest_epochs = task_epochs['rest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picks_meg = mne.pick_types(task_epochs.info, meg=True, ref_meg=False)\n",
    "# planning_X = planning_epochs.get_data()[:, picks_meg, :] # MEG signals: n_epochs, n_channels, n_times\n",
    "# rest_X = rest_epochs.get_data()[:, picks_meg, :]\n",
    "\n",
    "# X, y = (pca_data.copy(), y_raw.copy())\n",
    "# X_null = np.zeros_like(X[..., :11])\n",
    "# X = X[..., best_idx + -5:best_idx + 6] \n",
    "\n",
    "# rest_pca = pca.transform(rest_X)\n",
    "\n",
    "# for i in range(rest_X.shape[0]):\n",
    "#     for n, j in enumerate(np.random.randint(0, 589, 6)):\n",
    "#         if (i * 6) + n < X_null.shape[0] - 1:\n",
    "#             X_null[(i * 6) + n, ...] = rest_pca[i, :, j:j+11]\n",
    "            \n",
    "# X = np.vstack([X, X_null])\n",
    "# y = np.hstack([y, [99] * X_null.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from baggingPU import BaggingClassifierPU\n",
    "# bc = BaggingClassifierPU(\n",
    "#     LogisticRegression(solver='lbfgs', max_iter=1000, penalty='l2', C=0.1), n_estimators = 100, n_jobs = 1, \n",
    "#     max_samples = np.unique(y[y!=99], return_counts=True)[1].max()  # Each training sample will be balanced\n",
    "# )\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "# # clf = OneVsRestClassifier(bc)\n",
    "# clf = make_pipeline(FunctionTransformer(add_features, validate=False), RobustScaler(), \n",
    "#                     OneVsRestClassifier(bc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Show the results of the optimisation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('mean_test_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results of hyperparameter optimisation\n",
    "\n",
    "We can plot the results of the randomised search on a 3D mesh, with the two optimised parameters on the X and Y axes and accuracy on the Z axis. This is produced using [plotly](http://plot.ly/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_notebook_mode(connected=True)\n",
    "\n",
    "# trace = go.Mesh3d(x=results.param_logisticregression__C,\n",
    "#                   y=results.param_unsupervisedspatialfilter__estimator__n_components,\n",
    "#                   z=results.mean_test_score, \n",
    "#                   color='#275fb5', opacity=0.20)\n",
    "\n",
    "# layout = go.Layout(\n",
    "#     title='Hyperparameter optimisation results',\n",
    "#     autosize=True,\n",
    "#     width=700,\n",
    "#     height=700,\n",
    "#     scene = dict(\n",
    "#     xaxis = dict(\n",
    "#         title='Logistic regression C'),\n",
    "#     yaxis = dict(\n",
    "#         title='PCA N components'),\n",
    "#     zaxis = dict(\n",
    "#         title='Mean accuracy'),)\n",
    "# )\n",
    "\n",
    "# fig = go.Figure(data=[trace], layout=layout)\n",
    "# iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make confusion matrix with 5-fold CV\n",
    "\n",
    "The confusion matrix gives us an idea of whether any individual stimuli are being poorly decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.set_params(**random_search.best_params_)\n",
    "\n",
    "# Get predictions with 5 fold CV\n",
    "y_pred = cross_val_predict(clf, X, y, cv=3)\n",
    "y_pred_proba = cross_val_predict(clf, X, y, cv=5, method='predict_proba')\n",
    "mean_conf_mat = confusion_matrix(y, y_pred)\n",
    "mean_accuracy = accuracy_score(y, y_pred)\n",
    "mean_conf_mat = mean_conf_mat.astype('float') / mean_conf_mat.sum(axis=1)  # normalise\n",
    "\n",
    "print(\"Mean accuracy = {0}\".format(mean_accuracy))\n",
    "    \n",
    "# Plot mean confusion matrix\n",
    "plot_confusion_matrix(mean_conf_mat, title='Normalised confusion matrix, accuracy = {0}'.format(np.round(mean_accuracy, 2)))\n",
    "\n",
    "# # Save things\n",
    "# np.save(os.path.join(output_dir, 'localiser_classifier_performance', 'y', 'sub-{0}_localiser_y').format(session_id), y)\n",
    "# np.save(os.path.join(output_dir, 'localiser_classifier_performance', 'predictions', 'sub-{0}_localiser_predictions').format(session_id), y)\n",
    "# np.save(os.path.join(output_dir, 'localiser_classifier_performance', 'predictions_prob', 'sub-{0}_localiser_predictions_prob').format(session_id), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save components of the analysis for later use\n",
    "\n",
    "First save the classifier that was fit to all the localiser data using the best hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(random_search.best_estimator_ , os.path.join(output_dir, 'classifier', 'sub-{0}_classifier.pkl').format(session_id))\n",
    "# joblib.dump(pca, os.path.join(output_dir, 'classifier', 'sub-{0}_pca.pkl').format(session_id)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use papermill to save certain details, such as the mean accuracy, in the notebook so that we can read them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.record(\"mean_accuracy\", mean_accuracy)\n",
    "pm.record('best_C', random_search.best_params_['logisticregression__C'])\n",
    "# pm.record('best_n_components', random_search.best_params_['unsupervisedspatialfilter__estimator__n_components'])"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "psychopy3"
  },
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "mne"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nteract": {
   "version": "0.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
